---
layout: post
title: Week 6
---

### WebRTC + Web Speech API
In the past couple weeks, at some point, I did some work on getting the transcript page working with Web Speech API.  It was easy because there were guides that I followed in order to get it to work.  There were a couple obstacles that I faced, but I overcame it easily.  With a bit of understanding and knowledge that I had with Web Speech API, it was time for me to implement this into a WebRTC client.  In short, WebRTC platform offers many different methods of communications online, whether it be via video, audio, or instant messages.  In our project, we focus on the video/audio samples of WebRTC with the custom WebRTC implementation that Norman worked on to display Real-time Text (RTT) for conceptual purposes.  Getting the RTT to work with WebRTC is already complicated by itself with a lot of mathematical calculations for the positioning, sizing of the caption bar. This is mainly due to the videos' responsive design that size itself based on the display resolution and length (mobile phones, desktops, laptops, for example).  Thankfully, we do not have to go through all this trial and error just to get it right.  With that aside, Emelia and I could start focusing on getting our ASR engines (Web Speech API for me, MS Azure for her) to display the text it recognizes from speech (voiced) in the same area that RTT was used for. This saves us a lot of time and focus on the functionability of the ASRs. To say this aloud, it seems easy to get WebRTC ASR up and running inside the RTT caption bar.  Well this proved to be more difficult than I thought.  Since Emelia did get her ASR engine to work inside a RTT caption bar, I studied her code and tried to see how I could implement it similarly and differently because those two ASR engines are completely different. MS Azure seemed to work right out of the box because it did not take Emelia more time to get it to work properly as it did for me with the Web Speech API.  Web Speech API worked differently from MS Azure, this meant that some bits of the MS AZ code wouldn't work with Web Speech API, and I had to customize the code to fit the Web Speech API's needs.  It took me almost 3 work days to finally get it up and running.  Along the way, I faced some bugs, and one notable bug (Thanks to Norman for finding it) it was the invisible HTML element that did not get it to work.  If you were to read the source code, you would see the "transcript <div> element" in the code with nested code for <textarea> and <p> and various elements.  Well when you go to the webpage and inspect it with a browser's built-in developer tools, you would only see the "transcript <div> element" and NOTHING inside it.  This is what the web browser was interpreting the whole time.  So finally caught this notable bug, and afterwards, it worked properly.
  
### ASR switching


### Web Speech API transcript pop-up
